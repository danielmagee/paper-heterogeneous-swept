
\section{Methodology} \label{sec:hMethods}

% \subsection{Swept time-space decomposition}

The swept rule exhausts the domain of dependence---the portion of the space-time grid that can be solved given a set of initial values, referred to here as a ``block''---before passing the grid points on the borders of each process.
We refer to the program that implements the swept rule as \texttt{Swept}, and the program that uses naive domain decomposition, that is passing between processes at each timestep, is referred to as \texttt{Classic}.
This way the simulation may continue until no spatial points have available stencils; the required values may then be passed to the neighboring process (i.e., neighboring subdomain) in a single communication event.
Both Alhubail and Wang, and Magee and Niemeyer, provide detailed explanations and graphical depictions
of the swept rule in one dimension, for various architectures~\cite{alhubail:16jcp, OurJCP}.

The heterogeneous one-dimensional swept rule begins by partitioning the computational grid and
allocating space for the working array in each process.
In this case, the working array is of type \texttt{states}, a plain old data C struct that contains the
dependent and intermediate variables needed to continue the procedure from any time step.
Working array size is determined by the number of domains of dependence controlled by the process,
$nBlocks$, and the number of spatial points covered by a domain of dependence, $tpb$ (threads per block).
Here we use ``block'' to represent a domain of dependence; it comes from the GPU\slash CUDA
construct representing a collection of threads.
The program allocates space for $nBlocks \times tpb + (tpb+2)/2$ spatial points and initializes the
first $nBlocks \times tpb + 2$ points.
The initialized points require two extra slots so the edge domains
can build a full domain width on their first step.
Interior domains in the process share their edges with their neighbors; there is no risk of
race conditions since even the simplest numerical scheme requires at least two values in the
\texttt{state} struct, which allows the procedure to alternate reading and writing those values.
Therefore, even as a domain writes on an edge data point that its neighbor must read,
the value the neighbor requires is not modified.

The first cycle completes when each domain has progressed to the sub-time step $tpb/2$
where it has computed two values at the center of the spatial domain.
At this point each process passes the first $tpb/2 + 1$ values in its array to the left neighboring process.
Each process receives the neighbor's buffer and places it in the last $tpb/2 + 1$ slots; that is, starting at the $nBlocks \times tpb$ index.
It proceeds by performing the same computation on the centerpoints, starting at global index $tpb-1$ (adjusted index $tpb/2-1$), of the new array and filling in the uncomputed grid points at successive sub-time steps with a widening spatial window until it reaches a sub-time step that has not been explored at any spatial point and proceeds with a contracting window.
Geometrically, the first cycle completes a triangle, the second completes a diamond.
When the diamond is complete, it passes the last $tpb/2 + 1$ time steps in the array and inputs the received buffer starting at position 0.
Now it performs the diamond procedure again, this time the global and adjusted index are identical and it starts at index $tpb/2 - 1$.

The procedure continues in this fashion until the final time step is reached, at which point it stops after the expanding window reaches the domain width and outputs the solution which is now current at the same time step within and across all domains and processes.
Therefore, the triangle functions are only used twice if no intermediate time step results are output, the rest of the cycles are completed in a diamond shape.

\input{4_Implementation.tex}

\subsection{Experimental method} \label{sec:ExpMethod}

We endeavor to address the questions presented in Section~\ref{sec:obj1} by varying
three primary attributes of the decomposition: threads per block, GPU affinity, and grid size.
We repeatedly executed our two test equations, the heat and Euler equations, over the
experimental domain of these variables using \texttt{Swept} and \texttt{Classic}, exchanging borders every
sub-time step, decomposition methods.
In our program implementing the swept rule in one-dimension on heterogeneous systems, hSweep,
threads per block is synonymous with the size of the domain-of-dependence, but we refer to it using GPU terminology because each domain is launched as a block of threads on the GPU.
A block is an abstract grouping of threads that share an execution setting, a streaming multiprocessor, and access to a shared memory space, a portion of the GPU L1 cache.
hSweep uses the swept rule to avoid communication between devices and processes and exploits the
GPU memory hierarchy to operate on shared memory quantities closer to the processor.
Since this multi-level memory scheme influences the swept-rule performance and GPU execution, the resultant effects are difficult to predict.
The independent variables GPU affinity and grid size are more straightforward.
The grid size is the total number of spatial points in the simulation domain, and is provided
by the user; however, the program revises this number to provide a grid that fits the other program settings
that the grid must accommodate: the threads per block, GPU affinity, and number of processes.
The GPU affinity is the portion of the computational grid that is processed by the GPU,
expressed as a ratio of the number of domains-of-dependence assigned to the GPU to those assigned to a single MPI process (on a CPU core).
GPU affinity, like the other experimental variables, should be given as an integer, since we have
determined that it is beneficial for the GPU to handle a larger portion of the overall grid
than a single MPI process.

In our previous study of the swept rule~\cite{OurJCP}, the experimental domain was clearly
defined by the particular properties of GPU architecture.
Because a warp contains 32 threads and a block cannot exceed 1024 threads, here we constrained
the number of threads per block, which is also the width of the domain of dependence, in our experiments
to be a power of 2 from \numrange{32}{1024}.
To enforce regularity, we constrained our experimental problem size---the number of spatial points in the
grid---to be a power of 2 larger between \num{1024} and $2^{21}$.

Using CPU parallelism across 40 processes and GPU affinity as a variable of interest in this study, eliminates the potential for regularity in the experimental grid.
To remedy this, we relaxed the constraints on the experimental launch conditions so that the number of
threads per block is required to be a multiple of 32 from \numrange{32}{1024} rather than a power of two.
In addition, at runtime the program uses the number of processes, threads per block, GPU affinity,
and desired grid size to determine the closest grid size to the requested value that accommodates the constraints.
This results in different grid sizes for the same experimental settings.
To assess the performance at various settings, we interpolated each result to the requested grid size
from the actual grid size.

The addition of GPU affinity as an independent variable introduces further complication to the experimental domain.
While our experiments are constrained by GPU architecture in threads per block and by the number
of processes and blocks in problem size, we initially have no clear indication of what the
experimental limits of GPU affinity should be---so we took an iterative approach.
First, we ran a screening study and executed the programs over a broad range of conditions:
eight block sizes from \numrange{64}{768}, 11 GPU affinities from \numrange{0}{80}, and
four grid sizes from \numrange{5e5}{e7}.
This showed us that the best affinity for all the programs would likely fall between \numrange{20}{60}
and that all threads per block values could provide the best performance.
This was somewhat disappointing, since we had hoped to narrow the range for both GPU affinity and
threads per block further in order to experiment on a finer increment of grid size in a reasonable amount of time.
For the final experiment, we used the same block sizes, GPU affinity values from \numrange{20}{60}
in increments of 4, and seven grid sizes over the same range.

In this study, we solve the one-dimensional heat equation using a first-order forward in time, central in space
method and Euler equations for a shock wave using a second-order finite-volume scheme with minmod limiter.
Explanations of these methods can be found in the appendix of our previous paper~\cite{OurJCP}.
