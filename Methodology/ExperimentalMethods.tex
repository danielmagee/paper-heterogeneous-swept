
\subsection{Experimental method} \label{sec:ExpMethod}

We endeavor to address the questions presented in Section~\ref{sec:obj1} by varying
three elements of the decomposition: block size (the number of spatial points in each domain), GPU affinity (the number of spatial points assigned to a GPU as a multiple of the number assigned to a process), and grid size.
We repeatedly executed our two test equations, the heat equation and Euler equations, over the
experimental domain of these variables using the swept and classic, exchanging borders every
sub-timestep, decomposition methods.
In our swept program in one-dimension for heterogeneous systems, hSweep, the size of the domain-of-dependence or ``block'' is synonymous with threads per block, because each domain is launched as a block of threads on the GPU with each thread corresponding to a spatial point.
A block is an abstract grouping of threads that share an execution setting, a streaming multiprocessor, and access to a shared memory space, a portion of the GPU L1 cache.
hSweep uses the swept rule to avoid communication between devices and processes and exploits the
GPU memory hierarchy to operate on shared memory quantities closer to the processor.
Since this multi-level memory scheme influences the swept-rule performance and GPU execution, the resultant effects are difficult to predict.
The independent variables GPU affinity and grid size are more straightforward.
The grid size is the total number of spatial points in the simulation domain, and is provided
by the user; however, the program revises this number to provide a grid that fits the other program settings
that the grid must accommodate: the threads per block, GPU affinity, and number of processes.
The GPU affinity is the portion of the computational grid that is processed by the GPU,
expressed as a ratio of the number of domains-of-dependence assigned to the GPU to those assigned to a single MPI process (on a CPU core).
GPU affinity, like the other experimental variables, should be given as an integer, since we have
determined that it is beneficial for the GPU to handle a larger portion of the overall grid
than a single MPI process.

In our previous study of the swept rule~\cite{OurJCP}, the experimental domain was clearly
defined by the particular properties of GPU architecture.
Because a warp contains 32 threads and a block cannot exceed 1024 threads, here we constrained
the number of threads per block, which is also the width of the domain of dependence, in our experiments
to be a power of 2 from \numrange{32}{1024}.
To enforce regularity, we constrained our experimental problem size---the number of spatial points in the
grid---to be a power of 2 larger between \num{1024} and $2^{21}$.

Using CPU parallelism across 40 processes and GPU affinity as a variable of interest in this study, eliminates the potential for regularity in the experimental grid.
To remedy this, we relaxed the constraints on the experimental launch conditions so that the number of
threads per block is required to be a multiple of 32 from \numrange{32}{1024} rather than a power of two.
In addition, at runtime the program uses the number of processes, threads per block, GPU affinity,
and desired grid size to determine the closest grid size to the requested value that accommodates the constraints.
This results in different grid sizes for the same experimental settings.
To assess the performance at various settings, we interpolated each result to the requested grid size
from the actual grid size.

The addition of GPU affinity as an independent variable introduces further complication to the experimental domain.
While our experiments are constrained by GPU architecture in threads per block and by the number
of processes and blocks in problem size, we initially have no clear indication of what the
experimental limits of GPU affinity should be---so we took an iterative approach.
First, we ran a screening study and executed the programs over a broad range of conditions:
eight block sizes from \numrange{64}{768}, 11 GPU affinities from \numrange{0}{80}, and
four grid sizes from \numrange{5e5}{e7}.
This showed us that the best affinity for all the programs would likely fall between \numrange{20}{60}
and that all threads per block values could provide the best performance.
This was somewhat disappointing, since we had hoped to narrow the range for both GPU affinity and
threads per block further in order to experiment on a finer increment of grid size in a reasonable amount of time.
For the final experiment, we used the same block sizes, GPU affinity values from \numrange{20}{60}
in increments of 4, and seven grid sizes over the same range.

In this study, we solve the one-dimensional heat equation using a first-order forward in time, central in space
method and Euler equations for a shock wave using a second-order finite-volume scheme with minmod limiter.
Explanations of these methods can be found in the appendix of our previous paper~\cite{OurJCP}.